{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "# Turn inFile into a one hot label\n",
    "def createOneHot(inFile,amountOfClasses):\n",
    "    outFile = np.zeros((inFile.shape[0],inFile.shape[1], amountOfClasses));\n",
    "    for i in range(0,inFile.shape[0]):\n",
    "        for j in range(0,inFile.shape[1]):\n",
    "            #if inFile[i,j] != 0:\n",
    "            outFile[i,j,int(inFile[i,j])] = 1;\n",
    "    return outFile;\n",
    "\n",
    "# Turn 41 orthomosaics into one array. From 1 path with Bands 1 to 41\n",
    "def loadOneOrthomosaic(imagePath):\n",
    "    temp= np.asarray( Image.open( imagePath + 'Band1.png') );\n",
    "    inputData = np.zeros(( temp.shape[0], temp.shape[1], inputChannels),dtype=np.dtype('uint8'))\n",
    "    inputData[:,:,0] = temp[:,:]\n",
    "    for i in range(1,41):\n",
    "        temp= np.asarray( Image.open( imagePath + 'Band' + str(i+1) + '.png') );\n",
    "        inputData[:,:,i] = temp[:,:];\n",
    "    return np.expand_dims( inputData, axis=0)\n",
    "\n",
    "# Load one label from a .mat\n",
    "def loadOneLabel(imagePath):\n",
    "    temp = sio.loadmat(imagePath);\n",
    "    trainOneHotLabels =  createOneHot(temp['labeledPicture'], amountOfClasses);\n",
    "    return np.expand_dims( trainOneHotLabels, axis=0)\n",
    "\n",
    "# Read training data and labels for an array of path tuples, returns (data,labels)\n",
    "def readSet(paths):\n",
    "    numberOfImages = len(paths);\n",
    "    data = np.zeros( (numberOfImages,imageSize[1],imageSize[0],inputChannels), dtype=np.dtype('uint8'))\n",
    "    labels = np.zeros((numberOfImages,imageSize[1],imageSize[0],amountOfClasses), dtype=np.dtype('uint8'))\n",
    "    for i in range(0,numberOfImages):\n",
    "        data[i,:,:,:] = loadOneOrthomosaic(paths[i][0])\n",
    "        print('Loaded '+paths[i][0]+'.')\n",
    "        labels[i,:,:,:] = loadOneLabel(paths[i][1])\n",
    "        print('Loaded '+paths[i][1]+'.')\n",
    "    return data,labels\n",
    "\n",
    "def oneHotToBinary(oneHotLabel,threshold):\n",
    "    temp = np.zeros((oneHotLabel.shape[1],oneHotLabel.shape[2]), dtype=np.dtype('uint8') );\n",
    "    for i in range(0,oneHotLabel.shape[1]):\n",
    "        for j in range(0,oneHotLabel.shape[2]):\n",
    "            #temp[i,j] = np.argmax(oneHotLabel[0,i,j,:]) + 1;\n",
    "            if np.argmax(oneHotLabel[0,i,j,:])>threshold:\n",
    "                temp[i,j] = np.argmax(oneHotLabel[0,i,j,:])\n",
    "    return temp\n",
    "\n",
    "def predictAndSaveOneImage(inPath,outPath,FCNmodel,threshold):\n",
    "    testImage = loadOneOrthomosaic(inPath);\n",
    "    testLabelPredict = FCNmodel.predict(testImage);\n",
    "    testLabelPredictBinary = oneHotToBinary(testLabelPredict,threshold);\n",
    "    sio.savemat(outPath, mdict={'testLabelPredict': testLabelPredictBinary});\n",
    "\n",
    "def predictAndSaveSet(pathArray, FCNmodel,threshold):\n",
    "    numberOfImages = len(pathArray);\n",
    "    for i in range(0,numberOfImages):\n",
    "        predictAndSaveOneImage(pathArray[i][0], pathArray[i][1], FCNmodel,threshold)\n",
    "        print('Saved ' + pathArray[i][1])\n",
    "        \n",
    "def plotImage(image):\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "def getTrainPaths(paths):\n",
    "    folders = ['FIP','Ximea_Tamron']\n",
    "    fullPath = []\n",
    "    for imageType in paths:\n",
    "        for date in imageType:\n",
    "            temp = []\n",
    "            temp.append(basePath+'trainData/'+folders[paths.index(imageType)]+'/'+date+'/')\n",
    "            temp.append(basePath+'trainLabels/'+folders[paths.index(imageType)]+'_'+date+'.mat') \n",
    "            fullPath.append(temp)\n",
    "    return fullPath\n",
    "    \n",
    "def getTestPaths(paths):\n",
    "    folders = ['FIP','Ximea_Tamron']\n",
    "    fullPath = []\n",
    "    for imageType in paths:\n",
    "        for date in imageType:\n",
    "            temp = []\n",
    "            temp.append(basePath+'testData/'+folders[paths.index(imageType)]+'/'+date+'/')\n",
    "            temp.append(basePath+'testLabelPredict/'+folders[paths.index(imageType)]+'_'+date+'.mat') \n",
    "            fullPath.append(temp)\n",
    "    return fullPath\n",
    "\n",
    "def getSampleWeight(label):\n",
    "    counter = np.zeros(amountOfClasses)\n",
    "    numberOfElements = label.shape[0]*label.shape[1]\n",
    "    for i in range(0,label.shape[1]):\n",
    "        for j in range(0,label.shape[2]):\n",
    "            for k in range(0,label.shape[0]):\n",
    "                counter[ np.argmax(label[k,i,j]) ] += 1\n",
    "    counter = sum(counter)/counter\n",
    "    print('Class weights:')\n",
    "    print(counter)\n",
    "    return counter\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A weighted version of categorical_crossentropy for keras (2.0.6). This lets you apply a weight to unbalanced classes.\n",
    "@url: https://gist.github.com/wassname/ce364fddfc8a025bfab4348cf5de852d\n",
    "@author: wassname\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "basePath = '/Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/';\n",
    "\n",
    "trainSets = [\n",
    "    #FIP\n",
    "    [\n",
    "        '20170622'\n",
    "    ],\n",
    "    #Ximea_Tamron\n",
    "    [\n",
    "        '20170510',\n",
    "        '20170622'\n",
    "    ]\n",
    "]\n",
    "\n",
    "validationSets = [\n",
    "    #FIP\n",
    "    [\n",
    "        '20170531'\n",
    "    ],\n",
    "    #Ximea_Tamron\n",
    "    [\n",
    "    ]\n",
    "]\n",
    "\n",
    "testSets = [\n",
    "    #FIP\n",
    "    [\n",
    "        '20170531', \n",
    "        '20170622',\n",
    "        '20170531_cropped',\n",
    "        '20170622_cropped',\n",
    "        '20170802'\n",
    "    ],\n",
    "    #Ximea_Tamron\n",
    "    [\n",
    "        '20170510',\n",
    "        '20170622',\n",
    "        '20170510_cropped',\n",
    "        '20170622_cropped',\n",
    "        '20170613'\n",
    "    ]\n",
    "]\n",
    "    \n",
    "trainPaths = getTrainPaths(trainSets)\n",
    "validationPaths = getTrainPaths(validationSets)\n",
    "testPaths = getTestPaths(testSets)\n",
    "\n",
    "# Data parameters\n",
    "amountOfClasses = 9;\n",
    "inputChannels = 41;\n",
    "imageSize = [1600,1600]; # [X,Y]\n",
    "\n",
    "#numberOfEpochs = 20;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1600, 1600, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 1600, 1600, 6 23680       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 1600, 1600, 6 36928       conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 800, 800, 64) 0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 800, 800, 128 73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 800, 800, 128 147584      conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 400, 400, 128 0           conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 400, 400, 256 295168      max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 400, 400, 256 590080      conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 400, 400, 256 0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 800, 800, 256 0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 800, 800, 128 131200      up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 800, 800, 256 0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 800, 800, 128 295040      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 800, 800, 128 147584      conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 1600, 1600, 1 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 1600, 1600, 6 32832       up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1600, 1600, 1 0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 1600, 1600, 6 73792       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 1600, 1600, 6 36928       conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 1600, 1600, 9 585         conv2d_29[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,885,257\n",
      "Trainable params: 1,885,257\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Cropping2D, UpSampling2D,concatenate,Dropout\n",
    "\n",
    "#crop1 = Cropping2D(cropping=((0, 0), (0, 0)))(conv1) # cropping=((Top, Bottom), (Left, Right))\n",
    "\n",
    "model_inputs = Input(shape=(imageSize[0], imageSize[1], inputChannels))\n",
    "\n",
    "conv1 = Conv2D(64,3, activation='relu', padding='same')(model_inputs)\n",
    "conv1 = Conv2D(64,3, activation='relu', padding='same')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid')(conv1)\n",
    "\n",
    "conv2 = Conv2D(128,3, activation='relu', padding='same')(pool1)\n",
    "conv2 = Conv2D(128,3, activation='relu', padding='same')(conv2)\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid')(conv2)\n",
    "\n",
    "conv3 = Conv2D(256,3, activation='relu', padding='same')(pool2)\n",
    "conv3 = Conv2D(256,3, activation='relu', padding='same')(conv3)\n",
    "drop3 = Dropout(0.5)(conv3)\n",
    "\n",
    "upconv4 = Conv2D(128,2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(drop3))\n",
    "concat4 = concatenate([upconv4,conv2],axis=3)\n",
    "conv4 = Conv2D(128,3, activation='relu', padding='same')(concat4)\n",
    "conv4 = Conv2D(128,3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "upconv5 = Conv2D(64,2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(conv4))\n",
    "concat5 = concatenate([upconv5,conv1],axis=3)\n",
    "conv5 = Conv2D(64,3, activation='relu', padding='same')(concat5)\n",
    "conv5 = Conv2D(64,3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "conv6 = Conv2D(amountOfClasses, (1,1), activation='softmax', padding='valid')(conv5)\n",
    "\n",
    "\n",
    "UNetModel = Model(inputs = model_inputs, outputs = conv6)\n",
    "UNetModel.compile(optimizer='adagrad',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy']);\n",
    "print(UNetModel.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1600, 1600, 4 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 1600, 1600, 6 23680       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 1600, 1600, 6 36928       conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 800, 800, 64) 0           conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 800, 800, 128 73856       max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 800, 800, 128 147584      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 800, 800, 128 0           conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)  (None, 1600, 1600, 1 0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 1600, 1600, 6 32832       up_sampling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1600, 1600, 1 0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 1600, 1600, 6 73792       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 1600, 1600, 6 36928       conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 1600, 1600, 9 585         conv2d_27[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 426,185\n",
      "Trainable params: 426,185\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import *\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, Cropping2D, UpSampling2D,concatenate,Dropout\n",
    "\n",
    "model_inputs = Input(shape=(imageSize[0], imageSize[1], inputChannels))\n",
    "\n",
    "conv1 = Conv2D(64,3, activation='relu', padding='same')(model_inputs)\n",
    "conv1 = Conv2D(64,3, activation='relu', padding='same')(conv1)\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid')(conv1)\n",
    "\n",
    "conv2 = Conv2D(128,3, activation='relu', padding='same')(pool1)\n",
    "conv2 = Conv2D(128,3, activation='relu', padding='same')(conv2)\n",
    "drop2 = Dropout(0.5)(conv2)\n",
    "\n",
    "upconv3 = Conv2D(64,2, activation='relu', padding='same')(UpSampling2D(size=(2, 2))(drop2))\n",
    "concat3 = concatenate([upconv3,conv1],axis=3)\n",
    "conv3 = Conv2D(64,3, activation='relu', padding='same')(concat3)\n",
    "conv3 = Conv2D(64,3, activation='relu', padding='same')(conv3)\n",
    "\n",
    "conv4 = Conv2D(amountOfClasses, (1,1), activation='softmax', padding='valid')(conv3)\n",
    "\n",
    "UNetModel = Model(inputs = model_inputs, outputs = conv4)\n",
    "print(UNetModel.summary())\n",
    "\n",
    "weights = getSampleWeight(trainLabels)\n",
    "wLoss = weighted_categorical_crossentropy(weights)\n",
    "opt = keras.optimizers.SGD(lr=0.01)\n",
    "optAdam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "UNetModel.compile(loss= wLoss, optimizer= optAdam,  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainData/FIP/20170622/.\n",
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainLabels/FIP_20170622.mat.\n",
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainData/Ximea_Tamron/20170510/.\n",
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainLabels/Ximea_Tamron_20170510.mat.\n",
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainData/Ximea_Tamron/20170622/.\n",
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainLabels/Ximea_Tamron_20170622.mat.\n",
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainData/FIP/20170531/.\n",
      "Loaded /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/trainLabels/FIP_20170531.mat.\n"
     ]
    }
   ],
   "source": [
    "# Load the training data and labels\n",
    "trainData,trainLabels = readSet(trainPaths)\n",
    "validationData,validationLabels = readSet(validationPaths)\n",
    "\n",
    "#trainWeights = getSampleWeights(trainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 1600, 1600, 20)    840       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 1600, 1600, 9)     189       \n",
      "=================================================================\n",
      "Total params: 1,029\n",
      "Trainable params: 1,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelBasicNN = Sequential([\n",
    "    Conv2D(20, (1,1), input_shape=(imageSize[0], imageSize[1], inputChannels), activation='relu'),\n",
    "    Conv2D(amountOfClasses, (1,1), activation='softmax')\n",
    "])\n",
    "modelBasicNN.summary()\n",
    "\n",
    "#weights = getSampleWeight(trainLabels)\n",
    "weights = np.array([.01, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "wLoss = weighted_categorical_crossentropy(weights)\n",
    "opt = keras.optimizers.SGD(lr=0.01)\n",
    "optAdam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "modelBasicNN.compile(loss= wLoss, optimizer= optAdam,  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainData = trainData/255\n",
    "validationData = validationData/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 samples, validate on 1 samples\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 47s 16s/step - loss: 4.0212 - acc: 0.7204 - val_loss: 1.7578 - val_acc: 0.0020\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 43s 14s/step - loss: 3.9774 - acc: 0.0115 - val_loss: 1.6522 - val_acc: 0.0056\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 37s 12s/step - loss: 3.9724 - acc: 0.0117 - val_loss: 1.5649 - val_acc: 0.8962\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 34s 11s/step - loss: 3.9808 - acc: 0.7227 - val_loss: 1.5236 - val_acc: 0.8986\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 37s 12s/step - loss: 3.9846 - acc: 0.7228 - val_loss: 1.5138 - val_acc: 0.8993\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 38s 13s/step - loss: 3.9853 - acc: 0.7228 - val_loss: 1.5259 - val_acc: 0.8985\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 38s 13s/step - loss: 3.9840 - acc: 0.7229 - val_loss: 1.5567 - val_acc: 0.8968\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 40s 13s/step - loss: 3.9808 - acc: 0.7230 - val_loss: 1.6033 - val_acc: 0.8946\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 38s 13s/step - loss: 3.9756 - acc: 0.7231 - val_loss: 1.6583 - val_acc: 0.8924\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 34s 11s/step - loss: 3.9692 - acc: 0.7233 - val_loss: 1.7099 - val_acc: 0.8907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105c8e588>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "modelBasicNN.fit(trainData, trainLabels,validation_data=(validationData,validationLabels), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "UNetModel.save('networkModels/UNetModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/FIP_20170531.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/FIP_20170622.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/FIP_20170531_cropped.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/FIP_20170622_cropped.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/FIP_20170802.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/Ximea_Tamron_20170510.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/Ximea_Tamron_20170622.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/Ximea_Tamron_20170510_cropped.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/Ximea_Tamron_20170622_cropped.mat\n",
      "Saved /Volumes/mac_jannic_2017/thanujan/Datasets/xFcnClassifier/testLabelPredict/Ximea_Tamron_20170613.mat\n"
     ]
    }
   ],
   "source": [
    "# Use trained model to predict\n",
    "predictAndSaveSet(testPaths, modelBasicNN,.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trainData', 2519040144),\n",
       " ('test', 839680144),\n",
       " ('validationData', 839680144),\n",
       " ('trainLabels', 69120144),\n",
       " ('validationLabels', 23040144),\n",
       " ('Sequential', 2000),\n",
       " ('Conv2DTranspose', 1056),\n",
       " ('MaxPooling2D', 1056),\n",
       " ('Conv2D', 888),\n",
       " ('testPaths', 192),\n",
       " ('a', 168),\n",
       " ('weights', 168),\n",
       " ('categorical_crossentropy', 136),\n",
       " ('createOneHot', 136),\n",
       " ('getSampleWeight', 136),\n",
       " ('getSampleWeights', 136),\n",
       " ('getTestPaths', 136),\n",
       " ('getTrainPaths', 136),\n",
       " ('loadOneLabel', 136),\n",
       " ('loadOneOrthomosaic', 136),\n",
       " ('loss', 136),\n",
       " ('oneHotToBinary', 136),\n",
       " ('plotImage', 136),\n",
       " ('predictAndSaveOneImage', 136),\n",
       " ('predictAndSaveSet', 136),\n",
       " ('readSet', 136),\n",
       " ('scale_input', 136),\n",
       " ('weighted_categorical_crossentropy', 136),\n",
       " ('basePath', 107),\n",
       " ('trainPaths', 96),\n",
       " ('validationPaths', 96),\n",
       " ('Image', 80),\n",
       " ('K', 80),\n",
       " ('imageSize', 80),\n",
       " ('np', 80),\n",
       " ('plt', 80),\n",
       " ('sio', 80),\n",
       " ('testSets', 80),\n",
       " ('trainSets', 80),\n",
       " ('validationSets', 80),\n",
       " ('modelBasicNN', 56),\n",
       " ('opt', 56),\n",
       " ('amountOfClasses', 28),\n",
       " ('inputChannels', 28),\n",
       " ('numberOfEpochs', 28)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
